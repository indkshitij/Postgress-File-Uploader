
# ğŸ“¦ FileUploaderToPostgres

A **Python-based data ingestion utility** that automatically reads CSV and Excel files from a directory and uploads them into a PostgreSQL database with logging, error handling, and scalable batch inserts.

---

## ğŸš€ Features

* ğŸ“‚ Auto-detects **CSV / Excel** files from a data folder
* ğŸ§¹ Cleans and normalizes table names automatically
* ğŸ›¢ Uploads data into **PostgreSQL** using SQLAlchemy
* â• **Appends data** to existing tables (no overwrites)
* ğŸ“Š Handles large files with chunked batch inserts
* ğŸ“ Centralized logging (file + optional console)
* ğŸ” Secure DB connection via config file

---

## ğŸ“ Project Structure

```text
FileUploaderToPostgress/
â”‚
â”œâ”€â”€ data/                  # Input data files (.csv, .xlsx)
â”‚
â”œâ”€â”€ logs/                  # Auto-generated log files
â”‚   â””â”€â”€ inventory_ingest.log
â”‚
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ db_config.py       # Database engine configuration
â”‚   â”œâ”€â”€ ingestion.py       # Data ingestion logic
â”‚   â”œâ”€â”€ log_config.py      # Logger configuration
â”‚   â””â”€â”€ __pycache__/
â”‚
â”œâ”€â”€ uploader.ipynb         # Main ingestion runner (Notebook)
â””â”€â”€ README.md              # Project documentation
```

---

## âš™ï¸ Prerequisites

Make sure you have the following installed:

* Python **3.9+**
* PostgreSQL
* pip / conda

### Required Python Packages

```bash
pip install pandas sqlalchemy psycopg2-binary openpyxl
```

---

## ğŸ” Database Configuration

Update the database connection in `uploader.ipynb` or environment variable:

```python
DB_URL = "postgresql://username:password@host:port/database"
```

Example:

```python
postgresql://postgres:password@localhost:5432/blinkit_db
```

> âš ï¸ **Do NOT commit credentials** â€” keep `db_config.py` ignored in `.gitignore`.

---

## â–¶ï¸ How It Works

1. Place CSV / Excel files inside the `data/` folder
2. File names are converted into **table names**

   * Spaces & symbols â†’ `_`
   * Lowercased automatically
3. Tables are created if they donâ€™t exist
4. Data is **appended**, not overwritten
5. Logs are written to `logs/`

---

## ğŸ§  Core Logic Overview

### File Detection

```python
SUPPORTED_EXTENSIONS = (".csv", ".xlsx", ".xls")
```

### Table Name Cleaning

```python
orders-data.xlsx â†’ orders_data
```

### Ingestion Strategy

```python
df.to_sql(
    if_exists="append",
    chunksize=10000,
    method="multi"
)
```

âœ” Efficient
âœ” Scalable
âœ” Safe for production use

---

## ğŸ“ Logging

* Logs stored in `logs/<logger_name>.log`
* Includes timestamps, levels, and messages
* Console logging optional

Example log:

```text
2026-02-05 14:22:10 | INFO | inventory_ingest | Appended 12000 rows into orders
```

---

## âŒ Error Handling

* Each file is processed independently
* Errors are logged without stopping the pipeline
* Full stack trace captured for debugging

---

## ğŸ§ª Example Usage

```bash
# Activate environment
conda activate base

# Run notebook
jupyter notebook uploader.ipynb
```

OR convert to script later for cron / automation.

---

## ğŸ”® Future Enhancements

* âœ… Schema validation
* â± Incremental loading
* ğŸ” Duplicate detection
* â˜ï¸ Cloud storage (S3 / GCS)
* ğŸ³ Docker support
* ğŸ“ˆ Data quality checks

---
Built by Kshitij Singh

