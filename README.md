
# üì¶ FileUploaderToPostgres

A **Python-based data ingestion utility** that automatically reads CSV and Excel files from a directory and uploads them into a PostgreSQL database with logging, error handling, and scalable batch inserts.

---

## üöÄ Features

* üìÇ Auto-detects **CSV / Excel** files from a data folder
* üßπ Cleans and normalizes table names automatically
* üõ¢ Uploads data into **PostgreSQL** using SQLAlchemy
* ‚ûï **Appends data** to existing tables (no overwrites)
* üìä Handles large files with chunked batch inserts
* üìù Centralized logging (file + optional console)
* üîê Secure DB connection via config file

---

## üìÅ Project Structure

```text
FileUploaderToPostgress/
‚îÇ
‚îú‚îÄ‚îÄ data/                  # Input data files (.csv, .xlsx)
‚îÇ
‚îú‚îÄ‚îÄ logs/                  # Auto-generated log files
‚îÇ   ‚îî‚îÄ‚îÄ inventory_ingest.log
‚îÇ
‚îú‚îÄ‚îÄ utils/
‚îÇ   ‚îú‚îÄ‚îÄ db_config.py       # Database engine configuration
‚îÇ   ‚îú‚îÄ‚îÄ ingestion.py       # Data ingestion logic
‚îÇ   ‚îú‚îÄ‚îÄ log_config.py      # Logger configuration
‚îÇ   ‚îî‚îÄ‚îÄ __pycache__/
‚îÇ
‚îú‚îÄ‚îÄ uploader.ipynb         # Main ingestion runner (Notebook)
‚îî‚îÄ‚îÄ README.md              # Project documentation
```

---

## ‚öôÔ∏è Prerequisites

Make sure you have the following installed:

* Python **3.9+**
* PostgreSQL
* pip / conda

### Required Python Packages

```bash
pip install pandas sqlalchemy psycopg2-binary openpyxl
```

---

## üîê Database Configuration

Update the database connection in `uploader.ipynb` or environment variable:

```python
DB_URL = "postgresql://username:password@host:port/database"
```

Example:

```python
postgresql://postgres:password@localhost:5432/blinkit_db
```

> ‚ö†Ô∏è **Do NOT commit credentials** ‚Äî keep `db_config.py` ignored in `.gitignore`.

---

## ‚ñ∂Ô∏è How It Works

1. Place CSV / Excel files inside the `data/` folder
2. File names are converted into **table names**

   * Spaces & symbols ‚Üí `_`
   * Lowercased automatically
3. Tables are created if they don‚Äôt exist
4. Data is **appended**, not overwritten
5. Logs are written to `logs/`

---

## üß† Core Logic Overview

### File Detection

```python
SUPPORTED_EXTENSIONS = (".csv", ".xlsx", ".xls")
```

### Table Name Cleaning

```python
orders-data.xlsx ‚Üí orders_data
```

### Ingestion Strategy

```python
df.to_sql(
    if_exists="append",
    chunksize=10000,
    method="multi"
)
```

‚úî Efficient
‚úî Scalable
‚úî Safe for production use

---

## üìù Logging

* Logs stored in `logs/<logger_name>.log`
* Includes timestamps, levels, and messages
* Console logging optional

Example log:

```text
2026-02-05 14:22:10 | INFO | inventory_ingest | Appended 12000 rows into orders
```

---

## ‚ùå Error Handling

* Each file is processed independently
* Errors are logged without stopping the pipeline
* Full stack trace captured for debugging

---

## üß™ Example Usage

```bash
# Activate environment
conda activate base

# Run notebook
jupyter notebook uploader.ipynb
```

OR convert to script later for cron / automation.

---
Built by Kshitij Singh

